\documentclass[12pt]{extarticle}
\usepackage{polski}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}

\title{Modele generatywne na chmurach punktów 3D}
\author{Jakub Zadrożny}
\date{Maj 2019}

\begin{document}

\maketitle

\section{Wprowadzenie}
Reprezentacja punktów 3d jest bardzo ważna bo bla bla bla...

\section{Powiązane badania}

\subsection{Chmury punktów 3D}
\label{sec:related_pointnet}
Silnym narzędziem do pracy z chmurami punktów 3D jest architektura \textbf{PointNet} \cite{pointnet},
która umożliwa ekstrakcję cech (definiowanych przez wyuczalne parametry) z wejściowej chmury punktów.
Istotną zaletą tej atchitektury jest odporność na permutacje punktów chmury wejściowej.
Inne rozwiązania osiągnęły ten efekt przetwarzając chmury punktów na inne formaty (np. wykorzystujące voxele),
przez co rozmiar przetwarzanych danych wzrasta. Dzięki zastosowaniu serii splotów jednowymiarowych,
PointNet osiąga ten sam cel, zachowująć względną prostotę modelu.

\subsection{Modele generatywne}
Klasycznym wynikiem, który zapoczątkował prace nad modelami generatywnymi, jest autoenkoder wariacyjny
(\textbf{VAE}) \cite{vae}. Jego najważniejszą zaletą jest probabilistyczny charakter,
dzięki czemu możemy żądać, aby zmienne pośrednie (\textit{latent}) były zradnomizowane i pochodziły
z pewnego zadanego rozkładu prawdopodobieństwa (z pewnymi ograniczeniami).
Dzięki randomizacji zmiennych pośrednich uzyskujemy zagęszczenie przestrzeni reprezentacji,
przez co niemal każdy punkt może zostać zdekodowany na realistyczne dane.
Umożliwia to realizowanie zadań takich jak generowanie syntetycznych,
realistycznych danych lub interpolacja znaczeniowa obiektów.

\subsection{Półnadzorowana i nienadzorowana klasteryzacja}

\section{Metody}
\subsection{Autoenkoder wariacyjny}
\label{sec:vae_method}
Autoenkodery wariacyjne są dobrze znane i zbadane pod kątem niektórych zagadnień,
np. analizy obrazów. Jednak zastosowanie ich do chmur punktów 3D wymaga opracowania
dokładniejszej metody.

Dalej zakładamy, że dysponujemy zbiorem danych
$$
\mathcal{X} = \{x_i \in \mathbb{R}^{d}\}_{i \in I}
$$
dla pewnego $d$ -- wymiaru. Ponadto zakładamy, że dane te są obserwacjami
zmiennej losowej o rozkładzie następującej postaci
\begin{equation}
f(z, x; \theta) = f(z)f(x|z; \theta)
\label{eq:generative_process_factorization}
\end{equation}
gdzie $z \in \mathbb{R}^k$ dla pewnego $k$ (wymiar przestrzeni reprezentacji) -- zmienna pośrednia (\textit{latent}), $x \in \mathbb{R}^d$ -- zmienna obserwowana,
a $\Theta$ -- parametr rozkładu. Dodatkowo niech
\begin{equation}
\begin{split}
z &\sim \mathcal{N}(0, I_k) \\
x|z &\sim \mathcal{N}(\mu_x(z; \theta), \mu_\sigma(z;\theta)I_d)
\end{split}
\label{eq:generative_process_distribs}
\end{equation}
gdzie $\mu_x,\,\mu_{\sigma}$ są skomplikowanymi obliczeniami
wykonywanymi przez sieć neuronową sparametryzowaną przez $\theta$.

\subsubsection{ELBO}
Naszym celem jest odtworzenie parametrów rozkładu generującego $\theta$
oraz rozkładu $f(z|x; \theta)$, który nazywamy \textit{reprezentacją} danych
generowanych przez proces opsiany w
(\ref{eq:generative_process_factorization}) i (\ref{eq:generative_process_distribs}).

Niestety z powodu zastosowania skomplikowanych, nieliniowych
transformacji dokładne odtworzenie rozkładu $f(z|x; \theta)$ jest niemożliwe.
W tym celu wprowadzamy pewne przybliżenie tego rozkładu -- nazwijmy je
$g(z|x; \phi)$.

Niech $g(z|x; \theta)$ będzie gęstością
rozkładu normalnego ze średnią $\rho_x(x; \phi)$ i wariancją $\rho_{\sigma}(x; \phi)$,
gdzie $\rho_x,\,\rho_{\sigma}$ są reprezentowane przez sieci neuronowe
parametryzowane przez $\phi$. Wtedy
\begin{equation}
\begin{split}
KL(g(z|x;\phi) || f(z|x;\theta)) &= \mathbb{E}_{z\sim g(z|x;\phi)}\left[
-\log\frac{f(z|x;\theta)}{g(z|x;\phi)}\right]= \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z|x;\theta)f(x;\theta)}{g(z|x;\phi)f(x;\theta)}\right]= \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z|x;\theta)f(x;\theta)}{g(z|x;\phi)}+\log f(x;\theta)\right] \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right] +\log f(x;\theta)
\end{split}
\end{equation}
gdzie $KL(\cdotp || \cdotp)$ jest odległością Kullbacka-Leiblera \cite{kl-cost}.
Zatem
\begin{equation}
\log f(x;\theta) = KL(g(z|x;\phi) || f(z|x;\theta)) + \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right]
\end{equation}
Ponieważ $KL(\cdotp || \cdotp) \geq 0$, więc
\begin{equation}
\begin{split}
\log f(x;\theta)  &\geq \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right] = \\
&= \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(x|z;\theta)f(z)}{g(z|x;\phi)}\right] = \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x|z;\theta)\right] - \mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z)}{g(z|x;\phi)}\right] =\\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x|z;\theta)\right] - KL(g(z|x;\phi) || f(z))
\end{split}
\label{eq:elbo}
\end{equation}
Zatem dla dowolnego rozkładu aproksymującego $g(z|x;\phi)$ otrzymujemy dolne
ograniczenie na prawdopodobieństwo wygenerowania zaobserwowanych danych.
Dlatego część wzoru po prawej stronie od ostatniej równości
nazywamy \textbf{ELBO} (\textit{evidence lower bound}). Możemy zauważyć, że ostateczna
postać wzoru (\ref{eq:elbo}) składa się z dwóch części naturalnie odpowiadającymi dwóm
celom, które chcemy zoptymalizować: pierwszy składnik odpowiada
jakości rekonstrukcji obserwacji ze zmiennej ukrytej $z$
(dlatego nazywany jest kosztem rekonstrukcji),
natomiast drugi to odległość $KL$ rozkładu aproksymującego
$f(z|x;\theta)$ od naszego założenia na jego temat.

\subsubsection{Zadanie optymalizacyjne}
Chcemy znaleźć układ parametrów $<\theta,\,\phi>$, który daje najlepszą
gwarancję na prawdopodobieństwo wygenerowania zaobserwowanych danych (ELBO).
W tym celu posłużymy się lekko zmodyfikowanym algorytmem SGD.
Naszym zadaniem jest znalezienie
\begin{equation}
\label{eq:loss_f}
\begin{split}
\max_{\theta,\,\phi} \hat{\mathcal{L}}(\mathcal{X}, \theta, \phi) &= \sum_{i \in I}\mathcal{L}(x_i, \theta, \phi) = \\
&=\sum_{i \in I} \left( \mathbb{E}_{z\sim g(z|x_i;\phi)}\left[\log f(x_i|z;\theta)\right] - KL(g(z|x_i;\phi) || f(z)) \right)
\end{split}
\end{equation}
Ponieważ bardziej naturalnym zadaniem jest minimalizowanie funkcji kosztu,
to rozwiążemy równoważne zadanie znalezienia
\begin{equation}
\min_{\theta,\,\phi} -\hat{\mathcal{L}}(\mathcal{X}, \theta, \phi)
\end{equation}

Żeby posłużyć się algorytmem SGD musimy umieć wyliczać i różniczkować
oba składniki funckji $\mathcal{L}$.

\subsubsection{Koszt $KL$}
Odległość $KL$ dwóch rozkładów normalnych o następujących parametrach
\begin{equation*}
\begin{split}
\mathcal{N}_0 \sim \mathcal{N}(\mu_0, \Sigma_0) \\
\mathcal{N}_1 \sim \mathcal{N}(\mu_1, \Sigma_1)
\end{split}
\end{equation*}
dla pewncyh $\mu_0,\,\mu_1 \in \mathbb{R}^k,\ \Sigma_0, \Sigma_1 \in \mathbb{R}^{k \times k}$, wynosi
\begin{equation*}
KL(\mathcal{N}_0||\mathcal{N}_1) = \frac{1}{2} \left(\text{tr}(\Sigma_1^{-1}\Sigma_0)
+ (\mu_1-\mu_0)^T \Sigma_1^{-1} (\mu_1-\mu_0) -k + \log\frac{\det\Sigma_1}{\det\Sigma_0} \right)
\end{equation*}
Ponieważ zakładamy, że $f(z)$ jest rozkładem $z \sim \mathcal{N}(0, I_k)$, więc
\begin{equation}
KL(g(z|x;\phi) || f(z)) = \frac{1}{2}\sum_{i=1}^k
\left( \rho_x(x;\phi)_i^2 + \rho_\sigma(x;\phi)_i^2 - \log(\rho_\sigma(x;\phi)_i^2)-1 \right)
\label{eq:kl_loss}
\end{equation}
Wzór (\ref{eq:kl_loss}) można wyliczać i różniczkować analitycznie.

\subsubsection{Koszt rekonstrukcji}
Drugiego składnika funckji $\mathcal{L}$, czyli kosztu rekonstrukcji,
nie da się wyznaczyć analitycznie. Aby objeść ten problem, możemy metodą Monte Carlo
oszacować wartość oczekiwaną przez średnią
$$
\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x_i|z;\theta)\right] \sim
\frac{1}{m} \sum_{i=1}^m -\log f(x|z_i;\theta)
$$
gdzie $z_i \sim g(z|x;\phi)$. Taką wartość potrafimy już wyliczyć, ale nie
potrafimy propagować gradientu do parametrów $\phi$ przez
zaobserwowane wartości $z_i$.

Wprowadzimy repearametryzację zmiennych $z_i$ -- możemy zauważyć, że zmienna
$z_i=\rho_x(x;\phi)+\epsilon_i\rho_\sigma(x;\phi)$
gdzie $\epsilon_i \sim \mathcal{N}(0, I_k)$
ma rozkład $g(z|x;\phi)$ a ponadto możemy propagować gradient do parametrów $\phi$.
Otrzymaliśmy zatem następujące przybliżenie na $\mathcal{L}$
$$
\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x_i|z;\theta)\right] \sim
\frac{1}{m} \sum_{i=1}^m -\log f(x|z_i=\rho_x(x;\phi)+\epsilon_i\rho_\sigma(x;\phi);\theta)
$$
gdzie $\epsilon_i \sim \mathcal{N}(0, I_k)$

Ponieważ $x|z \sim \mathcal{N}(\mu_x(z; \theta), \mu_\sigma(z;\theta)I_d)$, więc
$$
-\log f(x|z;\theta)=\sum_{i=1}^d \left( \frac{1}{2}\log(2\pi) +\log(\mu_\sigma(z;\theta)_i)
+\frac{(x-\mu_x(z;\theta)_i)^2}{2\mu_\sigma(z;\theta)_i^2} \right)
$$
jednak metryka ta niezbyt dobrze nadaje się do chmur punktów, ponieważ np.
chcielibyśmy uznawać permutację punktów oryginalnej chmury za dobrą rekonstrukcję.
Dlatego zamiast wyliczać \textit{stricte} $\log f(x|z;\theta)$ skorzystamy ze zmodyfikowanego
\textit{Chamfer distance} danego wzorem
\begin{equation}
CD(\mathcal{X}_1,\mathcal{X}_2) = \sum_{x \in \mathcal{X}_1} \min_{y \in \mathcal{X}_2} ||x-y||_2^2
+ \sum_{x \in \mathcal{X}_2} \min_{y \in \mathcal{X}_1} ||x-y||_2^2
\label{eq:chamfer_distance}
\end{equation}
gdzie $\mathcal{X}_1,\,\mathcal{X}_2$ są zbiorami punktów wielowymiarowych.
Ściślej mówiąc, możemy potraktować $\mu_x(z;\theta) \in \mathbb{R}^{3 \cdotp m}$ jako chmurę
$m$ punktów trójwymiarowych, oznaczmy ją $\hat{y}$. Ponadto dla $y \in \hat{y}$ niech
$\sigma(y)$ oznacza 3-elementowy wektor wariancji $\mu_\sigma(z;\theta)$ utworzony ze
składowych odpowiadających $y$. Za koszt rekonstrukcji przyjmiemy
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \sum_{x \in \hat{x}} \min_{y \in \hat{y}}
\left( -\log p_{y,\sigma(y)}(x) \right) +\\
&+ \sum_{y \in \hat{y}} \min_{x \in \hat{x}} \left( -\log p_{x,\sigma(y)}(y) \right)
% \sum_{i=1}^3 \log(\mu_\sigma(z;\theta)_{y,i})+\frac{(x_i-y_i)^2}{2\mu_\sigma(z;\theta)_{y,i}^2} + \\
% &+ \sum_{x \in \mu_x(z|\theta) \min_{y \in \hat{x}}
% \sum_{i=1}^3 \log(\mu_\sigma(z;\theta)_{y,i})+\frac{(x_i-y_i)^2}{2\mu_\sigma(z;\theta)_{y,i}^2}
\end{split}
\end{equation}
gdzie $p_{v,s}(x)$ jest gęstością rozkładu normalnego o średniej $v$ i macierzy kowariancji
$sI$ w punkcie $x$.

Po usunięcu stałych wyrazów można to zapisać jako
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \sum_{x \in \hat{x}} \min_{y \in \hat{y}} \sum_{i=1}^3
\left( \log(\sigma(y)_i) + \frac{(x_i-y_i)^2}{2\sigma(y)_i^2} \right) +\\
&+ \sum_{y \in \hat{y}} \min_{x \in \hat{x}} \sum_{i=1}^3
\left( \log(\sigma(y)_i) + \frac{(x_i-y_i)^2}{2\sigma(y)_i^2} \right)
\end{split}
\label{eq:rec_loss}
\end{equation}
W obecnej wersji modelu dla uproszczenia przyjęto, że $\mu_\sigma(z;\theta) = \alpha$
dla wszystkich $z$ i niezależnie od parametrów $\theta$ (tzn. przyjęto stałą wariancję dla
danych wyjściowych). Wtedy wzór (\ref{eq:rec_loss}) upraszcza się do
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \frac{1}{2\alpha^2} \left(
\sum_{x \in \hat{x}} \min_{y \in \hat{y}} ||x-y||_2^2 +
\sum_{y \in \hat{y}} \min_{x \in \hat{x}} ||x-y||_2^2 \right) = \\
&= \frac{1}{2\alpha^2} CD(\hat{x}, \hat{y})
\end{split}
\end{equation}

\subsection{Model \textit{M2} Kingmy}
\subsection{GMVAE}

\section{Implementacja i architektura modeli}

\subsection{VAE}
% Dodac slowo o latent space - dim = 128
Metoda opsiana w sekcji \ref{sec:vae_method} zawiera abstrakcyjne obliczenia $\rho$
(nazywane dalej \textbf{enkoderem}) oraz $\mu$ (nazywane dalej \textbf{dekoderem}) realizowane przez
sieci neuronowe. W tym rozdziale przedstawiono konkretną architekturę nadaną tym obliczeniom.

\textbf{Enkoder} wykorzystuje przytoczoną w sekcji \ref{sec:related_pointnet} i wprowadzoną w \cite{pointnet}
architekturę PointNet do ektrakcji 1024 sparametryzowanych (wyuczalnych) cech (\textit{features})
z chmur wejściowych. Te cechy są dalej traktowane jako wejście do niewielkiej sieci MLP
(warstwa wejściowa -- 1024 neurony, jedna warstwa ukryta -- 1024 neurony i warstwa wyjściowa -- 256 neuronów).
Pomiędzy siecią PointNet i warstwami sieci MLP (z wyjątkiem ostatniej) znajduje się aktywacja ReLU
i warstwy \textit{batch normalization} \cite{batch_normalization}.
Ostatnia warstwa reprezentuje parametry rozkładów zmiennych pośrednich (po 128 liczb dla $\rho_x,\ \rho_\sigma$),
co oznacza, że wymiar zmiennej pośredniej wynosi 128.

\textbf{Dekoder} posiada znacznie prostszą architekturę -- jest to sieć MLP o 4 ukrytych warstwach
(kolejno 512, 1024, 1024 i 2048 neuronów) z aktywacjami ReLU (poza warstwą wyjściową).
Ostatnia warstwa składa się z $3*2048$ neuronów, co odpowiada wymiarowi wejściowych chmur punktów
i reprezentuje parametry $\mu_x$.

Pomiędzy enkoderem i dekoderem wykonywany jest trik repearametryzacyjny, tzn. wyliczenie zmiennej
pośredniej jako $\rho_x(x_i;\phi) + \epsilon \rho_\sigma(x_i;\phi)$,
gdzie $\epsilon \sim \mathcal{N}(0, I)$, co umożliwia różniczkowanie funkcji kosztu po
parametrach enkodera.

% Naturalnym pierwszym rozwiązaniem jest realizowanie
% tych obliczeń przez zwykłą sieć MLP (z dodatkiem \textit{batch normalization} [\ref{bib:batch_normalization}]).
% Obliczenia $\rho_x,\ \rho_\sigma$ są realizowane przez jedną sieć MLP, natomiast $\mu_x$ przez drugą (przypomnijmy,
% że dla uproszczenia przyjęto, że $\mu_\sigma$ jest stałe).
% Do testów przyjęto, że obie sieci $\rho_1,\ \mu_1$ są 4-warstwowe, w której każda ukryta warstwa składa się z 1024 neuronów.
%
% Dalej rozważono również bardziej skomplikowaną sieć $\rho$ opartą na architekturze \textit{Pointnet} [\ref{bib:pointnet}].
% Dokładniej, sieć $\rho_2$ jest początkowym fragmentem sieci \textit{Pointnet} wyszukującym poszczególne cechy (\textit{features})
% z doklejoną warstwą wyjściową (warstwa wyjściowa jest w pełni połączona z wyjściem sieci \textit{Pointnet}).
% Sieć $\mu_2$ pozostaje prostym MLP, dokładnie takim, jak $\mu_1$.
%
% W tabeli (\ref{tab:mlp_vs_pointnet}) znajduje się porównanie dwóch zaproponowanych powyżej architektur. Możemy zauważyć,
% że oba podejścia osiągają podobne wyniki na danych treningowych, jednak prosta sieć MLP znacznie bardziej \textit{overfituje}
% niż sieć oparta na \textit{Pointnecie}. Z racji tego, że drugi zestaw sieci znacznie lepiej się generalizuje, do dalszej
% pracy i testów przyjmiemy sieci oprate na \textit{Pointnecie}.

\subsection{M2}
\subsection{GMVAE}

\subsection{Zbiór danych i trenowanie}
Model został wytrenowany na jednej klasie obiektów wybranej z połączonych zbiorów ModelNet40 \cite{modelnet}
oraz ShapeNet \cite{shapenet} po uprzednim przekształceniu obu zbiorów do chmur punktów wymiaru 2048 \times 3
zgodnie z metodą opisaną w \cite{pc_convert_method}. Do treningu zbiór podzielono na części treningowe i testowe
w proporcjach 80\%-20\%.

Do optymalizacji funckji kosztu danej wzorem (\ref{eq:loss_f}) użyto metody ADAM \cite{adam}. Początkowe
\textit{learning rate} wynosiło $2 \times 10^{-4}$ i zmniejszało się dwukrotnie, co każde 500 epok.
Model trenowany był przez 3000 epok (duża liczba epok wynika z niewielkiej ilości próbek).

Na rys \ref{fig:loss_decrease} przedstawiona została zmiana całkowitej funkcji kosztu,
kosztu rekonstrukcji oraz kosztu KL w czasie. Możemy zaobserwować charakterystyczną dla autoenkoderów
wariacyjnych dynamikę kosztu KL -- podczas gdy koszt rekonstrukcji stale maleje, koszt KL na początku
szybko wzrasta i powoli spada z biegiem kolejnych epok. Początkowy wzrost kosztu KL odpowiada pakowaniu
przez model wielu informacji do zmiennej pośredniej, żeby uzyskać drastyczny spadek ogromnego kosztu rekonstrukcji.
Jednak gdy koszt rekonstrukcji spada, model zaczyna optymalizować informacje przechowywane w zmiennej pośredniej
tak, żeby jej rozład był podobny do $\mathcal{N}(0, I)$, czemu odpowiada powolny spadek kosztu KL.

\section{Wyniki eksperymentalne}
Na wytrenowanym modelu przeprowadzono kilka eksperymentów mających na celu sprawdzić zarówno zdolności modelu do dokładnej
rekonstrukcji, jak i jego możliwości generatywne.

\subsection{Rekonstrukcje}
Na rys \ref{fig:simple_reconstructions} znajdują się oryginalne chmury punktów ze zbioru \textit{Modelnet40} (po lewej)
wraz z chmurami zrekonstruowanymi przez dekoder na podstawie zmiennych pośrednich wyliczonych przez enkoder (po prawej).

Jedną z miar jakości rekonstrukcji autoenkoderów jest pokrycie (\textit{coverage}). Pokrycie definiujemy jako procent
próbek ze zbioru danych, dla którego najbliższa (w tym przypadku w sensie \textit{Chamfer Distance}) inna próbka
spośród całego zbioru dancyh oraz wszystkich rekonstrukcji pochodzi ze zbioru rekonstrukcji.
Dla wytrenowanego modelu pokrycie na rozważanym podzbiorze \textit{Modelnetu40} wynosi $...\%$.

Jedną z największych zalet VAE są jego zdolności generatywne. Klasycznym sposobem demonstracji zdolności generatywnych
modelu jest skonstruowanie takiej interpolacji pomiędzy dwoma różnymi obiektami, że każdy z jej kroków pośrednich
jest \textit{podobny} (wizualnie lub z użyciem metryki) do próbek z oryginalnego zbioru danych. Rys \ref{fig:interpolation}
przedstawiają interpolacje wykonane przez rozważany powyżej model. Można zaobserwować, że kolejne kroki interpolacji
coraz bardziej upodobniają obiekt źródłowy do docelowego, jednocześnie zachowując typowe cechy obiektów z oryginalnego
zbioru danych.

Własnością wyróżniającą VAE na tle innych enkoderów jest możliwość odgórnego zadania rozkładu zmiennych pośrednich,
który model będzie musiał osiągnąć. Dla rozważanego modelu zadano rozkład standardowy wielowymiarowy normalny
(o średniej w 0 i identycznościowej macierzy kowariancji). Dzięki temu, możemy tworzyć nowe, nieistniejące w zbiorze
danych próbki, przez wylosowanie zmiennej pośredniej z wybranego powyżej rozkładu i przekazanie jej do dekodera.
Rys. \ref{fig:random_latent} przedstawia chmury otrzymane w ten właśnie sposób. Możemy zauważyć, że powstałe
próbki dobrze pasują do reszty zbioru danych i ponadto prezentują dużą różnorodność (pochodzą z różnych podklas),
co świadczy o dużych możliwościach generatywnych modelu.

Rys \ref{fig:pca_tsne} przedstawiają wykresy median rozkładów zwróconych przez enkoder po zredukowaniu
do dwóch wymiarów metodami PCA (górny) i t-SNE (dolny). Możemy zauważyć, że na obu wykresach mediany rozmieszczone
są dość jednostajnie i gęsto na kole o środku w punkcie $(0, 0)$. Oznacza to, że reprezentacja próbek ze zbioru jest
\textit{ściśnięta} do zera i gęsta, co umożliwia przeprowadzenie interpolacji oraz tworzenie syntetycznych próbek
na podstawie wylosowanych zmiennych pośrednich (jak wyżej).

\section{Wnioski}

\bibliographystyle{ieeetr}
\bibliography{report}

\end{document}

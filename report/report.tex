\documentclass[12pt]{extarticle}
\usepackage{polski}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{cite}

\title{Modele generatywne na chmurach punktów 3D}
\author{Jakub Zadrożny}
\date{Maj 2019}

\begin{document}

\maketitle

\section{Podstawowe VAE}
W pierwszej części projektu zaimplementowany został podstawowy
autoenkoder wariacyjny. Dalej zakładamy, że dysponujemy zbiorem
danych treninngowych
$$
\mathcal{X} = \{x_i \in \mathbb{R}^{d}\}_{i \in I}
$$
dla pewnego $d$ -- wymiaru danych.

Ponadto zakładamy, że dane są obserwacjami zmiennej losowej
o rozkładzie następującej postaci
\begin{equation}
f(z, x; \theta) = f(z)f(x|z; \theta)
\label{eq:generative_process_factorization}
\end{equation}
Dodatkowo niech
\begin{equation}
\begin{split}
z &\sim \mathcal{N}(0, I_k) \\
x|z &\sim \mathcal{N}(\mu_x(z; \theta), \mu_\sigma(z;\theta)I_d)
\end{split}
\label{eq:generative_process_distribs}
\end{equation}
gdzie $\mu_x,\,\mu_{\sigma}$ są skomplikowanymi obliczeniami
wykonywanymi przez sieć neuronową sparametryzowaną przez $\theta$.

\subsection{ELBO}
Naszym celem jest odtworzenie parametrów rozkładu generującego $\theta$
oraz rozkładu $f(z|x; \theta)$, który nazywamy \textit{reprezentacją} danych
generowanych przez proces opsiany w
(\ref{eq:generative_process_factorization}) oraz (\ref{eq:generative_process_distribs}).

Niestety z powodu zastosowania skomplikowanych, nieliniowych
transformacji dokładne odtworzenie rozkładu $f(z|x; \theta)$ jest niemożliwe.
W tym celu wprowadzamy pewne przybliżenie tego rozkładu -- nazwijmy je
$g(z|x; \phi)$.

Niech $g(z|x; \theta)$ będzie gęstością
rozkładu normalnego ze średnią $\rho_x(x; \phi)$ i wariancją $\rho_{\sigma}(x; \phi)$,
gdzie $\rho_x,\,\rho_{\sigma}$ są reprezentowane przez sieci neuronowe
parametryzowane przez $\phi$. Wtedy
\begin{equation}
\begin{split}
D_{KL}(g(z|x;\phi) || f(z|x;\theta)) &= \mathbb{E}_{z\sim g(z|x;\phi)}\left[
-\log\frac{f(z|x;\theta)}{g(z|x;\phi)}\right]= \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z|x;\theta)f(x;\theta)}{g(z|x;\phi)f(x;\theta)}\right]= \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z|x;\theta)f(x;\theta)}{g(z|x;\phi)}\right] + \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x;\theta)\right] = \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right] +\log f(x;\theta)
\end{split}
\end{equation}
Zatem
\begin{equation}
\log f(x;\theta) = D_{KL}(g(z|x;\phi) || f(z|x;\theta)) + \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right]
\end{equation}
Ponieważ $D_{KL}(\cdotp || \cdotp) \geq 0$, więc
\begin{equation}
\begin{split}
\log f(x;\theta)  &\geq \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(z,x;\theta)}{g(z|x;\phi)}\right] = \\
&= \mathbb{E}_{z\sim g(z|x;\phi)}\left[\log\frac{f(x|z;\theta)f(z)}{g(z|x;\phi)}\right] = \\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x|z;\theta)\right] - \mathbb{E}_{z\sim g(z|x;\phi)}\left[-\log\frac{f(z)}{g(z|x;\phi)}\right] =\\
&=\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x|z;\theta)\right] - D_{KL}(g(z|x;\phi) || f(z))
\end{split}
\end{equation}
Zatem dla dowolnego rozkładu aproksymującego $g(z|x;\phi)$ otrzymujemy dolne
ograniczenie na prawdopodobieństwo wygenerowania zaobserwowanych danych.
Dlatego część wzoru po prawej stronie od ostatniej równości
nazywamy ELBO (\textit{evidence lower bound}). Ponadto pierwszy składnik odpowiada
jakości rekonstrukcji obserwacji ze zmiennej ukrytej $z$, więc nazywany jest
kosztem rekonstrukcji, natomiast drugi to odległość $KL$ rozkładu aproksymującego
$f(z|x;\theta)$ od naszego założenia na jego temat.

\subsection{Zadanie optymalizacyjne}
Chcemy znaleźć układ parametrów $<\theta,\,\phi>$, który daje najlepszą
gwarancję na prawdopodobieństwo wygenerowania zaobserwowanych danych (ELBO).
W tym celu posłużymy się lekko zmodyfikowanym algorytmem SGD.
Naszym zadaniem jest znalezienie
\begin{equation}
\begin{split}
\max_{\theta,\,\phi} \hat{\mathcal{L}}(\mathcal{X}, \theta, \phi) &= \sum_{i \in I}\mathcal{L}(x_i, \theta, \phi) = \\
&=\sum_{i \in I} \left( \mathbb{E}_{z\sim g(z|x_i;\phi)}\left[\log f(x_i|z;\theta)\right] - D_{KL}(g(z|x_i;\phi) || f(z)) \right)
\end{split}
\end{equation}
Ponieważ bardziej naturalnym zadaniem jest minimalizowanie funkcji kosztu,
to rozwiążemy równoważne zadanie znalezienia
\begin{equation}
\min_{\theta,\,\phi} -\hat{\mathcal{L}}(\mathcal{X}, \theta, \phi)
\end{equation}

Żeby posłużyć się algorytmem SGD musimy umieć wyliczać i różniczkować
oba składniki funckji $\mathcal(L)$.

\subsubsection{Koszt $KL$}
Odległość $KL$ dwóch rozkładów normalnych o następujących parametrach
\begin{equation*}
\begin{split}
\mathcal{N}_0 \sim \mathcal{N}(\mu_0, \Sigma_0) \\
\mathcal{N}_1 \sim \mathcal{N}(\mu_1, \Sigma_1)
\end{split}
\end{equation*}
dla pewncyh $\mu_0,\,\mu_1 \in \mathbb{R}^k,\ \Sigma_0, \Sigma_1 \in \mathbb{R}^{k \times k}$, wynosi
\begin{equation*}
D_{KL}(\mathcal{N}_0||\mathcal{N}_1) = \frac{1}{2} \left(\text{tr}(\Sigma_1^{-1}\Sigma_0)
+ (\mu_1-\mu_0)^T \Sigma_1^{-1} (\mu_1-\mu_0) -k + \log\frac{\det\Sigma_1}{\det\Sigma_0} \right)
\end{equation*}
Ponieważ zakładamy, że $f(z)$ jest rozkładem $z \sim \mathcal{N}(0, I_k)$, więc
\begin{equation}
D_{KL}(g(z|x;\phi) || f(z)) = \frac{1}{2}\sum_{i=1}^k
\left( \rho_x(x;\phi)_i^2 + \rho_\sigma(x;\phi)_i^2 - \log(\rho_\sigma(x;\phi)_i^2)-1 \right)
\label{eq:kl_loss}
\end{equation}
Wzór (\ref{eq:kl_loss}) można wyliczać i różniczkować analitycznie.

\subsubsection{Koszt rekonstrukcji}
Drugiego składnika funckji $\mathcal{L}$, czyli kosztu rekonstrukcji,
nie da się wyznaczyć analitycznie. Aby objeść ten problem, możemy metodą Monte Carlo
oszacować wartość oczekiwaną przez średnią
$$
\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x_i|z;\theta)\right] \sim
\frac{1}{m} \sum_{i=1}^m -\log f(x|z_i;\theta)
$$
gdzie $z_i \sim g(z|x;\phi)$. Taką wartość potrafimy już wyliczyć, ale nie
potrafimy propagować gradientu do parametrów $\phi$ przez
zaobserwowane wartości $z_i$.

Wprowadzimy repearametryzację zmiennych $z_i$ -- możemy zauważyć, że zmienna
$z_i=\rho_x(x;\phi)+\epsilon_i\rho_\sigma(x;\phi)$
gdzie $\epsilon_i \sim \mathcal{N}(0, I_k)$
ma rozkład $g(z|x;\phi)$ a ponadto możemy propagować gradient do parametrów $\phi$.
Otrzymaliśmy zatem następujące przybliżenie na $\mathcal{L}$
$$
\mathbb{E}_{z\sim g(z|x;\phi)}\left[\log f(x_i|z;\theta)\right] \sim
\frac{1}{m} \sum_{i=1}^m -\log f(x|z_i=\rho_x(x;\phi)+\epsilon_i\rho_\sigma(x;\phi);\theta)
$$
gdzie $\epsilon_i \sim \mathcal{N}(0, I_k)$

Ponieważ $x|z \sim \mathcal{N}(\mu_x(z; \theta), \mu_\sigma(z;\theta)I_d)$, więc
$$
-\log f(x|z;\theta)=\sum_{i=1}^d \left( \frac{1}{2}\log(2\pi) +\log(\mu_\sigma(z;\theta)_i)
+\frac{(x-\mu_x(z;\theta)_i)^2}{2\mu_\sigma(z;\theta)_i^2} \right)
$$
jednak metryka ta niezbyt dobrze nadaje się do chmur punktów, ponieważ np.
chcielibyśmy uznawać permutację punktów oryginalnej chmury za dobrą rekonstrukcję.
Dlatego zamiast wyliczać \textit{stricte} $\log f(x|z;\theta)$ skorzystamy ze zmodyfikowanego
\textit{Chamfer distance} danego wzorem
\begin{equation}
CD(\mathcal{X}_1,\mathcal{X}_2) = \sum_{x \in \mathcal{X}_1} \min_{y \in \mathcal{X}_2} ||x-y||_2^2
+ \sum_{x \in \mathcal{X}_2} \min_{y \in \mathcal{X}_1} ||x-y||_2^2
\label{eq:chamfer_distance}
\end{equation}
gdzie $\mathcal{X}_1,\,\mathcal{X}_2$ są zbiorami punktów wielowymiarowych.
Ściślej mówiąc, możemy potraktować $\mu_x(z;\theta) \in \mathbb{R}^{3 \cdotp m}$ jako chmurę
$m$ punktów trójwymiarowych, oznaczmy ją $\hat{y}$. Ponadto dla $y \in \hat{y}$ niech
$\sigma(y)$ oznacza 3-elementowy wektor wariancji $\mu_\sigma(z;\theta)$ utworzony ze
składowych odpowiadających $y$. Za koszt rekonstrukcji przyjmiemy
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \sum_{x \in \hat{x}} \min_{y \in \hat{y}}
\left( -\log p_{y,\sigma(y)}(x) \right) +\\
&+ \sum_{y \in \hat{y}} \min_{x \in \hat{x}} \left( -\log p_{x,\sigma(y)}(y) \right)
% \sum_{i=1}^3 \log(\mu_\sigma(z;\theta)_{y,i})+\frac{(x_i-y_i)^2}{2\mu_\sigma(z;\theta)_{y,i}^2} + \\
% &+ \sum_{x \in \mu_x(z|\theta) \min_{y \in \hat{x}}
% \sum_{i=1}^3 \log(\mu_\sigma(z;\theta)_{y,i})+\frac{(x_i-y_i)^2}{2\mu_\sigma(z;\theta)_{y,i}^2}
\end{split}
\end{equation}
gdzie $p_{v,s}(x)$ jest gęstością rozkładu normalnego o średniej $v$ i macierzy kowariancji
$sI$ w punkcie $x$.

Po usunięcu stałych wyrazów można to zapisać jako
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \sum_{x \in \hat{x}} \min_{y \in \hat{y}} \sum_{i=1}^3
\left( \log(\sigma(y)_i) + \frac{(x_i-y_i)^2}{2\sigma(y)_i^2} \right) +\\
&+ \sum_{y \in \hat{y}} \min_{x \in \hat{x}} \sum_{i=1}^3
\left( \log(\sigma(y)_i) + \frac{(x_i-y_i)^2}{2\sigma(y)_i^2} \right)
\end{split}
\label{eq:rec_loss}
\end{equation}
W obecnej wersji modelu dla uproszczenia przyjęto, że $\mu_\sigma(z;\theta) = \alpha$
dla wszystkich $z$ i niezależnie od parametrów $\theta$ (tzn. przyjęto stałą wariancję dla
danych wyjściowych). Wtedy wzór (\ref{eq:rec_loss}) upraszcza się do
\begin{equation}
\begin{split}
\mathcal{L}_{rec}(\hat{x}|z,\theta,\phi) &= \frac{1}{2\alpha^2} \left(
\sum_{x \in \hat{x}} \min_{y \in \hat{y}} ||x-y||_2^2 +
\sum_{y \in \hat{y}} \min_{x \in \hat{x}} ||x-y||_2^2 \right) = \\
&= \frac{1}{2\alpha^2} CD(\hat{x}, \hat{y})
\end{split}
\end{equation}

\subsection{Implementacja praktyczna}
W tej sekcji przedstawiona została praktyczna implementacja teoretycznego zadania optymalizacyjnego
opsianego w sekcji \ref{sec:optim_task}. Ściślej, zaimplementowany został model uczący się reprezentacji
danych ze zbioru \textit{Modelnet40} (\ref{bib:modelnet40}).

\subsubsection{\textit{Modelnet40}}

\subsubsection{Przestrzeń \textit{reprezentacji danych}}
W rozważanej implementacji przyjęto 128-wymiarową przestrzeń \textit{reprezentacji danych}.

\subsubsection{Architektura modelu}
W opisie rozwiązywanego zagadnienia optymalizacyjnego (sekcja \ref{sec:optim_task}) użyte zostały
abstrakcyjne obliczenia $\rho$ oraz $\mu$ realizowane przez sieci neuronowe. Do praktycznej implementacji
należy nadać tym obliczeniom pewną architekturę. Naturalnym pierwszym rozwiązaniem jest realizowanie
tych obliczeń przez zwykłą sieć MLP (z dodatkiem \textit{batch normalization} [\ref{bib:batch_normalization}]).
Obliczenia $\rho_x,\ \rho_\sigma$ są realizowane przez jedną sieć MLP, natomiast $\mu_x$ przez drugą (przypomnijmy,
że dla uproszczenia przyjęto, że $\mu_\sigma$ jest stałe).
Do testów przyjęto, że obie sieci $\rho_1,\ \mu_1$ są 4-warstwowe, w której każda ukryta warstwa składa się z 1024 neuronów.

Dalej rozważono również bardziej skomplikowaną sieć $\rho$ opartą na architekturze \textit{Pointnet} [\ref{bib:pointnet}].
Dokładniej, sieć $\rho_2$ jest początkowym fragmentem sieci \textit{Pointnet} wyszukującym poszczególne cechy (\textit{features})
z doklejoną warstwą wyjściową (warstwa wyjściowa jest w pełni połączona z wyjściem sieci \textit{Pointnet}).
Sieć $\mu_2$ pozostaje prostym MLP, dokładnie takim, jak $\mu_1$.

W tabeli (\ref{tab:mlp_vs_pointnet}) znajduje się porównanie dwóch zaproponowanych powyżej architektur. Możemy zauważyć,
że oba podejścia osiągają podobne wyniki na danych treningowych, jednak prosta sieć MLP znacznie bardziej \textit{overfituje}
niż sieć oparta na \textit{Pointnecie}. Z racji tego, że drugi zestaw sieci znacznie lepiej się generalizuje, do dalszej
pracy i testów przyjmiemy sieci oprate na \textit{Pointnecie}.

\subsubsection{Trenowanie modelu}
Do optymalizacji funckji kosztu danej wzorem (\ref{eq:loss_f}) użyto metody ADAM [\ref{bib:adam}]. Początkowe learning rate
wynosiło $0.0002$ i zmniejszało się o połowę, co każde 1000 epok. Model trenowany był przez 5000 epok (duża liczba epok
wynika z niewielkiej ilości próbek). Na rys \ref{fig:loss_decrease} przedstawiona została zmiana całkowitej funkcji kosztu,
kosztu rekonstrukcji oraz kosztu KL w czasie.

\subsubsection{Wyniki eksperymentalne}
Na wytrenowanym modelu przeprowadzono kilka eksperymentów mających na celu sprawdzić zarówno zdolności modelu do dokładnej
rekonstrukcji, jak i jego możliwości generatywne.

Na rys \ref{fig:simple_reconstructions} znajdują się oryginalne chmury punktów ze zbioru \textit{Modelnet40} (po lewej)
wraz z chmurami zrekonstruowanymi przez dekoder na podstawie zmiennych pośrednich wyliczonych przez enkoder (po prawej).

Jedną z miar jakości rekonstrukcji autoenkoderów jest pokrycie (\textit{coverage}). Pokrycie definiujemy jako procent
próbek ze zbioru danych, dla którego najbliższa (w tym przypadku w sensie \textit{Chamfer Distance}) inna próbka
spośród całego zbioru dancyh oraz wszystkich rekonstrukcji pochodzi ze zbioru rekonstrukcji.
Dla wytrenowanego modelu pokrycie na rozważanym podzbiorze \textit{Modelnetu40} wynosi $...\%$.

Jedną z największych zalet VAE są jego zdolności generatywne. Klasycznym sposobem demonstracji zdolności generatywnych
modelu jest skonstruowanie takiej interpolacji pomiędzy dwoma różnymi obiektami, że każdy z jej kroków pośrednich
jest \textit{podobny} (wizualnie lub z użyciem metryki) do próbek z oryginalnego zbioru danych. Rys \ref{fig:interpolation}
przedstawiają interpolacje wykonane przez rozważany powyżej model. Można zaobserwować, że kolejne kroki interpolacji
coraz bardziej upodobniają obiekt źródłowy do docelowego, jednocześnie zachowując typowe cechy obiektów z oryginalnego
zbioru danych.

Własnością wyróżniającą VAE na tle innych enkoderów jest możliwość odgórnego zadania rozkładu zmiennych pośrednich,
który model będzie musiał osiągnąć. Dla rozważanego modelu zadano rozkład standardowy wielowymiarowy normalny
(o średniej w 0 i identycznościowej macierzy kowariancji). Dzięki temu, możemy tworzyć nowe, nieistniejące w zbiorze
danych próbki, przez wylosowanie zmiennej pośredniej z wybranego powyżej rozkładu i przekazanie jej do dekodera.
Rys. \ref{fig:random_latent} przedstawia chmury otrzymane w ten właśnie sposób. Możemy zauważyć, że powstałe
próbki dobrze pasują do reszty zbioru danych i ponadto prezentują dużą różnorodność (pochodzą z różnych podklas),
co świadczy o dużych możliwościach generatywnych modelu.

Rys \ref{fig:pca_tsne} przedstawiają wykresy median rozkładów zwróconych przez enkoder po zredukowaniu
do dwóch wymiarów metodami PCA (górny) i t-SNE (dolny). Możemy zauważyć, że na obu wykresach mediany rozmieszczone
są dość jednostajnie i gęsto na kole o środku w punkcie $(0, 0)$. Oznacza to, że reprezentacja próbek ze zbioru jest
\textit{ściśnięta} do zera i gęsta, co umożliwia przeprowadzenie interpolacji oraz tworzenie syntetycznych próbek
na podstawie wylosowanych zmiennych pośrednich (jak wyżej).

% Appendix architektura
% Appendix parametry

\bibliographystyle{plain}
\bibliography{M335}

\end{document}
